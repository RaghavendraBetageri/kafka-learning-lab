# Cluster Configuration Deep Dive

> **Purpose**: Comprehensive reference for understanding every configuration parameter in our 3-broker Kafka cluster.  
> **Audience**: Data engineers learning Kafka fundamentals through hands-on local experimentation.  
> **Environment**: macOS M1, 16GB RAM, local development setup.

---

## Quick Navigation

- [Architecture Overview](#architecture-overview)
- [ZooKeeper Configuration](#zookeeper-configuration)
- [Kafka Broker Configuration](#kafka-broker-configuration)
  - [Server Basics](#server-basics)
  - [Socket Server Settings](#socket-server-settings)
  - [Log Basics](#log-basics)
  - [Log Retention Policy](#log-retention-policy)
  - [Replication Settings](#replication-settings)
  - [ZooKeeper Settings](#zookeeper-settings-in-broker)
  - [Group Coordinator Settings](#group-coordinator-settings)
- [Configuration Differences Across Brokers](#configuration-differences-across-brokers)
- [M1 Mac Optimization Tips](#m1-mac-optimization-tips)
- [Common Mistakes & Troubleshooting](#common-mistakes--troubleshooting)
- [What Happens When...](#what-happens-when)
- [Testing Your Configuration](#testing-your-configuration)

---

## Architecture Overview

### Cluster Topology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ZooKeeper                            â”‚
â”‚                    localhost:2181                           â”‚
â”‚          (Cluster Metadata & Coordination)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚             â”‚             â”‚
              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
              â”‚   â”‚   Registration    â”‚   â”‚
              â”‚   â”‚   & Heartbeats    â”‚   â”‚
              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
              â”‚             â”‚             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Broker 0    â”‚  â”‚ Broker 1 â”‚  â”‚  Broker 2  â”‚
    â”‚  Port: 9092   â”‚  â”‚Port: 9093â”‚  â”‚Port: 9094  â”‚
    â”‚  ID: 0        â”‚  â”‚ ID: 1    â”‚  â”‚  ID: 2     â”‚
    â”‚               â”‚  â”‚          â”‚  â”‚            â”‚
    â”‚  /tmp/kafka-  â”‚  â”‚ /tmp/    â”‚  â”‚  /tmp/     â”‚
    â”‚  logs-0       â”‚  â”‚ kafka-   â”‚  â”‚  kafka-    â”‚
    â”‚               â”‚  â”‚ logs-1   â”‚  â”‚  logs-2    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–²                â–²              â–²
           â”‚                â”‚              â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Replication (Factor: 2)
```

### Data Flow Example: Topic with 3 Partitions, Replication Factor 2

```
Topic: user-events (3 partitions, replication factor: 2)

Partition 0:  Leader: Broker 0  â†’  Follower: Broker 1
              â””â”€â”€ Stores on: /tmp/kafka-logs-0/user-events-0/
                             /tmp/kafka-logs-1/user-events-0/

Partition 1:  Leader: Broker 1  â†’  Follower: Broker 2
              â””â”€â”€ Stores on: /tmp/kafka-logs-1/user-events-1/
                             /tmp/kafka-logs-2/user-events-1/

Partition 2:  Leader: Broker 2  â†’  Follower: Broker 0
              â””â”€â”€ Stores on: /tmp/kafka-logs-2/user-events-2/
                             /tmp/kafka-logs-0/user-events-2/

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Each broker is both LEADER (for some) and               â”‚
â”‚  FOLLOWER (for others) - distributed leadership!         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ZooKeeper Configuration

**File**: `config/zookeeper.properties`

### What is ZooKeeper's Role?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ZooKeeper Manages:                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Broker Registry (which brokers are alive?)           â”‚
â”‚  â€¢ Topic Configurations (partitions, replication)       â”‚
â”‚  â€¢ Partition Leadership (who's the leader?)             â”‚
â”‚  â€¢ Consumer Group Offsets (legacy, pre-0.9)             â”‚
â”‚  â€¢ Controller Election (one broker manages cluster)     â”‚
â”‚  â€¢ ACLs & Quotas                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<details>
<summary><strong>ğŸ“‚ dataDir=/tmp/zookeeper</strong></summary>

**What it does**: Directory where ZooKeeper stores snapshots and transaction logs.

**Why it matters**: 
- Contains persistent cluster state
- All broker metadata, topic configs, partition assignments stored here
- âš ï¸ **Warning**: `/tmp/` is cleared on reboot - production should use `/var/lib/zookeeper`

**Storage Pattern**:
```
/tmp/zookeeper/
â”œâ”€â”€ version-2/
â”‚   â”œâ”€â”€ snapshot.0           # State snapshots
â”‚   â”œâ”€â”€ log.1                # Transaction log
â”‚   â””â”€â”€ log.2
â””â”€â”€ myid                      # ZooKeeper node ID (for ensemble)
```

**Our Setup**: Using `/tmp/` for easy cleanup during learning experiments.

**Test It**:
```bash
# Check ZooKeeper data directory
ls -lah /tmp/zookeeper/version-2/

# Monitor growth during operations
watch -n 2 du -sh /tmp/zookeeper
```

</details>

<details>
<summary><strong>ğŸ”Œ clientPort=2181</strong></summary>

**What it does**: Port where ZooKeeper listens for client connections.

**Why it matters**:
- Kafka brokers connect on this port
- Standard ZooKeeper port (like 80 for HTTP)
- All 3 brokers connect to `localhost:2181`

**Technical Detail**: 
- This is for client API, not inter-ZooKeeper communication
- In multi-node ZooKeeper ensemble, peer communication uses `2888` (follower) and `3888` (leader election)

**Test It**:
```bash
# Verify ZooKeeper is listening
lsof -i :2181

# Connect to ZooKeeper shell
kafka/bin/zookeeper-shell.sh localhost:2181

# Inside shell, explore structure:
ls /
ls /brokers
ls /brokers/ids
```

</details>

<details>
<summary><strong>ğŸ”— maxClientCnxns=0</strong></summary>

**What it does**: Maximum concurrent connections from single IP (`0` = unlimited).

**Why it matters**:
- Our 3 brokers each maintain 1 connection = 3 total
- Admin tools (kafka-topics.sh, etc.) create temporary connections
- `0` prevents connection limit errors during learning

**Production Consideration**: Set to `60` to prevent connection flooding attacks.

**Test It**:
```bash
# Check which brokers are connected to ZooKeeper
kafka/bin/zookeeper-shell.sh localhost:2181 ls /brokers/ids
# Expected output: [0, 1, 2]

# Check ZooKeeper is listening and accepting connections
lsof -i :2181
# Should show ZooKeeper LISTEN + multiple ESTABLISHED connections

# Count active connections (each broker creates 1 connection)
lsof -i :2181 | grep ESTABLISHED | wc -l
# Expected: 6 (3 from broker side + 3 from ZooKeeper side = bidirectional)

# View connection details
lsof -i :2181 | grep ESTABLISHED
# Shows PID and port pairs for each broker connection
```

**Understanding the Connection Count**:
```
Why 6 connections instead of 3?

TCP connections are bidirectional, so lsof shows both ends:

ZooKeeper side (PID 71112):
  â”œâ”€ localhost:2181 â†’ localhost:49381 (Broker 0)
  â”œâ”€ localhost:2181 â†’ localhost:49388 (Broker 1) 
  â””â”€ localhost:2181 â†’ localhost:49392 (Broker 2)

Broker side (3 different PIDs):
  â”œâ”€ localhost:49381 â†’ localhost:2181 (Broker 0)
  â”œâ”€ localhost:49388 â†’ localhost:2181 (Broker 1)
  â””â”€ localhost:49392 â†’ localhost:2181 (Broker 2)

Total: 6 entries in lsof (but actually 3 physical connections)
```

</details>

<details>
<summary><strong>âš™ï¸ admin.enableServer=false</strong></summary>

**What it does**: Disables embedded Jetty HTTP server for admin commands.

**Why it matters**:
- Admin server runs on port 8080 by default
- Provides metrics, health checks, management endpoints
- Disabled to reduce resource usage for learning

**If Enabled, You'd Get**:
```
http://localhost:8080/commands
http://localhost:8080/commands/stat
http://localhost:8080/commands/conf
```

</details>

<details>
<summary><strong>â±ï¸ tickTime=2000</strong></summary>

**What it does**: Basic time unit (2000ms = 2 seconds) for heartbeats and timeouts.

**Why it matters**:
- Fundamental timing unit in ZooKeeper
- One "tick" = 2 seconds
- Other timeouts are expressed as multiples of `tickTime`

**Example Calculation**:
```
tickTime = 2000ms
initLimit = 10 ticks
Actual timeout = 10 Ã— 2000ms = 20 seconds
```

**Heartbeat Flow**:
```
Client â”€â”€[heartbeat]â”€â”€> ZooKeeper    (every 2 seconds)
        <â”€â”€[ack]â”€â”€â”€â”€â”€â”€â”€â”€

If no heartbeat for (sessionTimeout / tickTime) ticks:
  â†’ Session expires, client considered dead
```

</details>

<details>
<summary><strong>ğŸš€ initLimit=10</strong></summary>

**What it does**: Maximum ticks (10 Ã— 2s = 20s) for follower to initially sync with leader.

**Why it matters**:
- During startup, followers need time to sync state
- If sync takes > 20 seconds â†’ follower marked as failed
- âš ï¸ Note: We have 1 ZooKeeper, so this doesn't apply to our setup (good practice to include)

**Formula**: `initLimit_seconds = initLimit Ã— tickTime / 1000`

**Production Scenario**: 
- Large datasets might need `initLimit=20` (40 seconds)
- Fast networks can use `initLimit=5` (10 seconds)

</details>

<details>
<summary><strong>ğŸ’“ syncLimit=5</strong></summary>

**What it does**: Maximum ticks (5 Ã— 2s = 10s) a follower can lag behind leader during normal operation.

**Why it matters**:
- Heartbeat timeout for in-sync replicas
- If follower doesn't respond within 10s â†’ removed from ensemble
- Prevents slow/failed nodes from blocking cluster

**Difference from initLimit**:
```
initLimit:  For initial data sync (startup)
syncLimit:  For ongoing heartbeats (normal operation)
```

</details>

---

## Kafka Broker Configuration

**Files**: `config/server-0.properties`, `config/server-1.properties`, `config/server-2.properties`

### Broker Responsibilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Kafka Broker Functions                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Storage       â”‚ Write messages to disk (logs)      â”‚
â”‚  2. Serving       â”‚ Handle producer/consumer requests  â”‚
â”‚  3. Replication   â”‚ Leader/follower for partitions     â”‚
â”‚  4. Coordination  â”‚ Register with ZooKeeper            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Server Basics

<details>
<summary><strong>ğŸ†” broker.id=0 (or 1, or 2)</strong></summary>

**What it does**: Unique integer identifier for each broker.

**Why it matters**:
- Distinguishes brokers in the cluster
- Used in partition assignment, leader election, replication
- ZooKeeper tracks brokers by ID

**Our Setup**:
```
Broker 0: broker.id=0
Broker 1: broker.id=1
Broker 2: broker.id=2
```

**Auto-Generation**: If not set, Kafka auto-generates starting from `reserved.broker.max.id + 1` (default 1000).

**Best Practice**: Explicit IDs are clearer for troubleshooting.

**Test It**:
```bash
# Check which brokers are registered
kafka/bin/zookeeper-shell.sh localhost:2181 ls /brokers/ids

# Get broker details
kafka/bin/zookeeper-shell.sh localhost:2181 get /brokers/ids/0
```

</details>

---

## Socket Server Settings

<details>
<summary><strong>ğŸŒ listeners=PLAINTEXT://localhost:9092</strong></summary>

**What it does**: URI where broker listens for connections.

**Format Breakdown**:
```
PLAINTEXT://localhost:9092
    â†“          â†“        â†“
Security   Hostname  Port
Protocol
```

**Why it matters**:
- Producers/consumers connect here to send/receive messages
- Each broker needs unique port
- `localhost` = only local connections (production uses actual hostname/IP)

**Our Setup**:
```
Broker 0: PLAINTEXT://localhost:9092
Broker 1: PLAINTEXT://localhost:9093
Broker 2: PLAINTEXT://localhost:9094
```

**Other Security Protocols**:
```
SSL://              - Encryption only
SASL_PLAINTEXT://   - Authentication only
SASL_SSL://         - Both encryption + authentication
```

**Test It**:
```bash
# Verify broker is listening
lsof -i :9092
lsof -i :9093
lsof -i :9094

# Test connection
telnet localhost 9092
```

</details>

<details>
<summary><strong>ğŸ“¢ advertised.listeners=PLAINTEXT://localhost:9092</strong></summary>

**What it does**: Address published to ZooKeeper for clients to use.

**Why it matters**:
- Client workflow:
  ```
  1. Client â†’ ZooKeeper: "Where are the brokers?"
  2. ZooKeeper â†’ Client: "Here are advertised.listeners"
  3. Client â†’ Broker: Connects using advertised address
  ```

**Critical for Docker/Cloud**:
```
# Container scenario:
listeners=PLAINTEXT://0.0.0.0:9092              # Bind internally
advertised.listeners=PLAINTEXT://kafka1.company.com:9092  # Public address
```

**Our Setup**: Since localhost, both are the same.

**Common Mistake**:
```
âŒ listeners=PLAINTEXT://0.0.0.0:9092
   advertised.listeners=PLAINTEXT://0.0.0.0:9092
   
   â†’ Clients can't connect! (0.0.0.0 is not routable)

âœ… listeners=PLAINTEXT://0.0.0.0:9092
   advertised.listeners=PLAINTEXT://kafka1.prod.com:9092
```

**Test It**:
```bash
# Get broker metadata (includes advertised listeners)
kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092

# Verify advertised address in ZooKeeper
kafka/bin/zookeeper-shell.sh localhost:2181 get /brokers/ids/0
```

</details>

<details>
<summary><strong>ğŸ§µ num.network.threads=3</strong></summary>

**What it does**: Threads handling network I/O (receiving/sending data).

**Why it matters**:
- Handle socket operations (accept, read, write)
- More threads = higher concurrent connection throughput
- Too many â†’ context-switching overhead

**Thread Workflow**:
```
Producer â”€â”€â”
           â”œâ”€â†’ [Network Thread 1] â”€â”€â†’ Request Queue
Producer â”€â”€â”¤
           â”œâ”€â†’ [Network Thread 2] â”€â”€â†’ Request Queue
Consumer â”€â”€â”˜
           â””â”€â†’ [Network Thread 3] â”€â”€â†’ Request Queue
```

**Default**: 3 (good for moderate load)
**Production**: 8-16 for high-throughput scenarios
**M1 Mac**: 3-4 is optimal for local testing

**Test It**:
```bash
# Monitor thread count
jstack <broker_pid> | grep "kafka-network-thread" | wc -l

```

</details>

<details>
<summary><strong>ğŸ’¾ num.io.threads=8</strong></summary>

**What it does**: Threads doing disk I/O (reading/writing log files).

**Why it matters**:
- Handle actual disk operations
- Disk I/O often the bottleneck in Kafka
- Parallelizes disk access across partitions

**I/O Flow**:
```
Request Queue
     â†“
[IO Thread 1] â†’ Write to partition-0 log
[IO Thread 2] â†’ Write to partition-1 log
[IO Thread 3] â†’ Read from partition-2 log
[IO Thread 4] â†’ Write to partition-3 log
     â‹®
[IO Thread 8]
```

**Default**: 8
**Production**: Should â‰¥ number of disks for optimal parallelization
**M1 Mac SSD**: 6-8 is good (M1 has fast unified storage)

**Tuning for M1**:
```properties
# M1 Mac optimization
num.io.threads=6                    # Slightly lower for single SSD
num.network.threads=4               # Match efficiency cores
```

</details>

<details>
<summary><strong>ğŸ“¨ socket.send.buffer.bytes=102400</strong></summary>

**What it does**: TCP send buffer size (100 KB).

**Why it matters**:
- OS-level buffer for sending data to clients
- Larger buffers = better throughput on high-latency networks
- Trade-off: More memory per connection

**Buffer Behavior**:
```
Broker Memory â†’ [100 KB Send Buffer] â†’ Network â†’ Client
                      â†“
            If buffer fills, slow down!
```

**Default**: 102400 bytes (100 KB)
**Production**: 1-2 MB for high-throughput
**M1 Mac**: Default is fine for local testing

</details>

<details>
<summary><strong>ğŸ“¥ socket.receive.buffer.bytes=102400</strong></summary>

**What it does**: TCP receive buffer size (100 KB).

**Why it matters**:
- Buffers incoming data from clients
- Absorbs bursty traffic
- Prevents dropped packets during spikes

**Default**: 102400 bytes (100 KB)

</details>

<details>
<summary><strong>ğŸ“¦ socket.request.max.bytes=104857600</strong></summary>

**What it does**: Maximum single request size (100 MB).

**Why it matters**:
- Prevents clients from overwhelming broker with huge requests
- Includes message batches + metadata
- Must be > your max message size

**Request Types Limited**:
```
â€¢ Produce requests (message batches)
â€¢ Fetch requests (consumer reads)
â€¢ Metadata requests
```

**Default**: 104857600 bytes (100 MB)
**Production**: Adjust based on message sizes

**Common Issue**:
```
âŒ Error: "Message batch size exceeds socket.request.max.bytes"
   
âœ… Solution: Increase socket.request.max.bytes OR
             Reduce producer batch.size
```

</details>

---

## Log Basics

> **"Logs" in Kafka = append-only data structures where messages are stored** (NOT debugging logs!)

<details>
<summary><strong>ğŸ“ log.dirs=/tmp/kafka-logs-0</strong></summary>

**What it does**: Directory where Kafka stores partition data.

**Why it matters**:
- Actual message data lives here
- Each partition = subdirectory
- **Must be unique per broker** to avoid data corruption

**Directory Structure**:
```
/tmp/kafka-logs-0/
â”œâ”€â”€ user-events-0/              # Partition 0 of topic "user-events"
â”‚   â”œâ”€â”€ 00000000000000000000.log     # Segment file
â”‚   â”œâ”€â”€ 00000000000000000000.index   # Offset index
â”‚   â”œâ”€â”€ 00000000000000000000.timeindex
â”‚   â””â”€â”€ leader-epoch-checkpoint
â”œâ”€â”€ user-events-1/
â”œâ”€â”€ __consumer_offsets-0/       # Internal topic
â””â”€â”€ meta.properties
```

**Our Setup**:
```
Broker 0: /tmp/kafka-logs-0
Broker 1: /tmp/kafka-logs-1
Broker 2: /tmp/kafka-logs-2
```

**Production Multi-Disk**:
```properties
log.dirs=/disk1/kafka,/disk2/kafka,/disk3/kafka

# Kafka distributes partitions across disks:
/disk1/kafka/topic-0/
/disk2/kafka/topic-1/
/disk3/kafka/topic-2/
```

**âš ï¸ Warning**: Never point multiple brokers to same log.dirs!

**Test It**:
```bash
# Explore partition structure
ls -lah /tmp/kafka-logs-0/

# Find all partitions for a topic
find /tmp/kafka-logs-* -name "user-events-*" -type d

# Check disk usage
du -sh /tmp/kafka-logs-*
```

</details>

<details>
<summary><strong>ğŸ“Š num.partitions=3</strong></summary>

**What it does**: Default partitions for new topics (if not specified).

**Why it matters**:
- Partitions = unit of parallelism
- More partitions = more consumer parallelism = higher throughput
- Too many = overhead (more files, more metadata memory)

**Partition Distribution**:
```
Topic: user-events (3 partitions, replication: 2)

Partition 0: Broker 0 (leader), Broker 1 (follower)
Partition 1: Broker 1 (leader), Broker 2 (follower)
Partition 2: Broker 2 (leader), Broker 0 (follower)

Each broker handles different partitions â†’ load balanced!
```

**Our Setup**: 3 partitions matches 3 brokers (good for even distribution).

**Choosing Partition Count**:
```
âœ… Good: partition_count = broker_count Ã— N
   (where N = 1, 2, 3... for even distribution)

Examples:
3 brokers â†’ 3, 6, 9, 12 partitions
5 brokers â†’ 5, 10, 15, 20 partitions
```

**Test It**:
```bash
# Create topic with default partitions
kafka/bin/kafka-topics.sh --create \
  --topic test-default \
  --bootstrap-server localhost:9092

# Verify partition count
kafka/bin/kafka-topics.sh --describe \
  --topic test-default \
  --bootstrap-server localhost:9092
```

</details>

<details>
<summary><strong>ğŸ”§ num.recovery.threads.per.data.dir=1</strong></summary>

**What it does**: Threads per log directory for startup recovery.

**Why it matters**:
- On broker start, validates/recovers partition logs
- More threads = faster startup after crashes
- Parallelizes across partitions

**Recovery Process**:
```
Broker Startup:
  â†“
[Recovery Thread 1] â†’ Check partition-0 logs
[Recovery Thread 2] â†’ Check partition-1 logs  (if set to 2)
[Recovery Thread 3] â†’ Check partition-2 logs  (if set to 3)
  â†“
Broker Ready!
```

**Default**: 1 (conservative)
**Production**: Set to CPU cores for faster recovery
**M1 Mac**: 4-6 (match performance cores)

</details>

---

## Log Retention Policy

<details>
<summary><strong>â³ log.retention.hours=168</strong></summary>

**What it does**: Keep messages for 168 hours (7 days) before deletion.

**Why it matters**:
- Kafka doesn't delete after consumption
- Multiple consumers can read same data
- After 7 days, old messages deleted to free space

**Retention Timeline**:
```
Message arrives â†’ Day 0
                  â†“
Consumer reads  â†’ Day 1 (message still exists!)
                  â†“
Another consumerâ†’ Day 3 (message still exists!)
                  â†“
7 days pass     â†’ Day 7 (message deleted)
```

**Priority Hierarchy** (if multiple set):
```
log.retention.ms          (highest priority)
log.retention.minutes
log.retention.hours       (lowest priority)
```

**Use Cases**:
```
Real-time analytics:   log.retention.hours=2
Audit logs:           log.retention.hours=720  (30 days)
Event sourcing:       log.retention.ms=-1      (infinite)
Our learning:         log.retention.hours=168  (7 days)
```

**Test It**:
```bash
# Create topic with custom retention
kafka/bin/kafka-topics.sh --create \
  --topic short-lived \
  --bootstrap-server localhost:9092 \
  --config retention.ms=60000  # 1 minute for testing

# Produce message
echo "test" | kafka/bin/kafka-console-producer.sh \
  --topic short-lived \
  --bootstrap-server localhost:9092

# Wait 61 seconds, then check (message should be gone)
kafka/bin/kafka-console-consumer.sh \
  --topic short-lived \
  --from-beginning \
  --bootstrap-server localhost:9092
```

</details>

<details>
<summary><strong>ğŸ“„ log.segment.bytes=1073741824</strong></summary>

**What it does**: Max size of single log segment file (1 GB).

**Why it matters**:
- Kafka doesn't use one giant file per partition
- Splits into 1 GB segments
- Retention works on segment granularity

**Segment Structure**:
```
/tmp/kafka-logs-0/user-events-0/
â”œâ”€â”€ 00000000000000000000.log  (1 GB, closed)
â”œâ”€â”€ 00000000000001234567.log  (1 GB, closed)
â”œâ”€â”€ 00000000000002468135.log  (500 MB, active â† writing here)
â””â”€â”€ ...

Retention deletes entire segments, not individual messages!
```

**Retention Example**:
```
Segment 1: All messages older than 7 days â†’ DELETED
Segment 2: Mix of old/new messages â†’ KEPT (not deleted yet!)
Segment 3: All new messages â†’ KEPT
```

**Trade-off**:
```
Larger segments (1 GB):
  âœ… Fewer files
  âŒ Coarser retention (might keep old data longer)

Smaller segments (100 MB):
  âœ… Finer-grained retention
  âŒ More files, more overhead
```

**M1 Mac Consideration**: Default 1 GB is fine (M1 SSD handles many files well).


</details>

<details>
<summary><strong>ğŸ”„ log.retention.check.interval.ms=300000</strong></summary>

**What it does**: How often (5 minutes) Kafka checks for segments to delete.

**Why it matters**:
- Deletion isn't instant when retention expires
- Background thread checks every 5 minutes
- Balances prompt deletion vs. CPU overhead

**Deletion Timeline**:
```
10:00 AM: Message expires (7 days old)
10:03 AM: Still on disk (next check at 10:05)
10:05 AM: Deletion thread runs â†’ Segment deleted
```

**Production**: Usually fine at default.

</details>

---

## Replication Settings

> **MOST CRITICAL for fault tolerance and data durability!**

### Replication Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Topic: payments | 3 partitions | Replication: 2       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Partition 0:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  replicates  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Broker 0   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  Broker 1   â”‚
  â”‚  (LEADER)   â”‚              â”‚  (FOLLOWER) â”‚
  â”‚  [WRITE]    â”‚â—„â”€â”€â”€ ack â”€â”€â”€â”€â”€â”‚  [READ-ONLY]â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
  Serves all reads/writes for Partition 0

If Broker 0 fails:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Broker 1   â”‚  â† Promoted to LEADER
  â”‚  (NEW LEADER)â”‚     (automatic!)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<details>
<summary><strong>ğŸ” default.replication.factor=2</strong></summary>

**What it does**: New topics get 2 copies of each partition (if not specified).

**Why it matters**:
```
Replication Factor 1:  No fault tolerance (data lost if broker dies)
Replication Factor 2:  Survives 1 broker failure
Replication Factor 3:  Survives 2 broker failures
```

**Formula**: `max_failures_tolerated = replication_factor - 1`

**Our Setup**: Factor 2 with 3 brokers
```
âœ… Benefit: Can lose 1 broker, cluster still works
âœ… Trade-off: Uses 2Ã— storage
```

**Why Not Factor 3?**
- Demonstrates you can lose 1 broker and still function
- Saves disk space for learning
- In production, factor 3 is recommended

**Storage Impact**:
```
10 GB topic with replication factor 2:
  â†’ Total cluster storage: 20 GB
  
10 GB topic with replication factor 3:
  â†’ Total cluster storage: 30 GB
```

**Test It**:
```bash
# Create topic with explicit replication
kafka/bin/kafka-topics.sh --create \
  --topic critical-data \
  --partitions 3 \
  --replication-factor 2 \
  --bootstrap-server localhost:9092

# Verify replication
kafka/bin/kafka-topics.sh --describe \
  --topic critical-data \
  --bootstrap-server localhost:9092

# Output shows:
# Topic: critical-data  Partition: 0  Leader: 0  Replicas: 0,1  Isr: 0,1
#                       Partition: 1  Leader: 1  Replicas: 1,2  Isr: 1,2
#                       Partition: 2  Leader: 2  Replicas: 2,0  Isr: 2,0
```

</details>

<details>
<summary><strong>ğŸ›¡ï¸ min.insync.replicas=1</strong></summary>

**What it does**: Minimum replicas that must acknowledge a write (when producer uses `acks=all`).

**Why it matters**: Controls durability vs. availability trade-off.

**Producer Acks Levels**:
```
acks=0:  Fire and forget (no acknowledgment)
         â”œâ”€ Fastest
         â””â”€ Can lose data

acks=1:  Leader acknowledges
         â”œâ”€ Medium speed
         â””â”€ Data on leader, but might not be replicated

acks=all: Wait for min.insync.replicas
          â”œâ”€ Slowest
          â””â”€ Strongest durability
```

**Our Setup**: `min.insync.replicas=1` with `acks=all`
```
Write Flow:
  Producer â”€â”€[message]â”€â”€> Broker 0 (leader)
                          â†“
                     [writes to disk]
                          â†“
  Producer <â”€â”€[ack]â”€â”€â”€â”€â”€â”€ Leader (doesn't wait for follower)
                          â†“
                    [async replication to follower]

âš ï¸ Weak durability! If leader crashes before replication, data lost.
```

**Better Production Setting**:
```properties
default.replication.factor=3
min.insync.replicas=2

# This means:
# - Write succeeds only if leader + at least 1 follower have data
# - Can tolerate 1 broker failure and still accept writes
```

**Trade-off Matrix**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ min.insync.rep  â”‚ Durability â”‚ Availability â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1 (our setup)   â”‚ Weak       â”‚ High         â”‚
â”‚ 2 (recommended) â”‚ Strong     â”‚ Medium       â”‚
â”‚ 3 (paranoid)    â”‚ Strongest  â”‚ Low          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Our Setup Uses 1**: 
- Allows writes even if only 1 broker up
- Good for learning/experimentation
- Not recommended for production

**Test It**:
```bash
# Test with acks=all
kafka/bin/kafka-console-producer.sh \
  --topic test \
  --bootstrap-server localhost:9092 \
  --producer-property acks=all

# Stop a broker, verify writes still work (min.insync.replicas=1)
# Stop 2 brokers, writes should fail (only 1 broker left)
```

</details>

<details>
<summary><strong>ğŸ“ offsets.topic.replication.factor=2</strong></summary>

**What it does**: Replication for internal `__consumer_offsets` topic.

**Why it matters**:
- Kafka stores consumer group offsets in this special topic
- Tracks "which messages have been consumed"
- If lost, consumers lose their position â†’ re-read everything or skip data

**Offset Storage**:
```
Consumer Group: analytics-team
  â””â”€â”€ __consumer_offsets topic stores:
      user-events partition 0: offset 12345
      user-events partition 1: offset 67890
      user-events partition 2: offset 54321
```

**Our Setup**: Replication 2 = survives 1 broker failure.
**Production**: Set to 3 for critical workloads.

**Test It**:
```bash
# Check __consumer_offsets topic
kafka/bin/kafka-topics.sh --describe \
  --topic __consumer_offsets \
  --bootstrap-server localhost:9092

# View consumer offsets
kafka/bin/kafka-consumer-groups.sh --describe \
  --group my-group \
  --bootstrap-server localhost:9092
```

</details>

<details>
<summary><strong>ğŸ”’ transaction.state.log.replication.factor=2</strong></summary>

**What it does**: Replication for internal `__transaction_state` topic.

**Why it matters**:
- Used for exactly-once semantics (EOS) transactions
- Stores transaction coordinator state
- Critical for transactional producers/consumers

**When You Need This**:
```
Transactional Producer:
  producer.initTransactions();
  producer.beginTransaction();
  producer.send(record1);
  producer.send(record2);
  producer.commitTransaction();  â† Uses __transaction_state
```

**Production**: Set to 3 for transactional workloads.

</details>

<details>
<summary><strong>ğŸ¯ transaction.state.log.min.isr=1</strong></summary>

**What it does**: Minimum in-sync replicas for transaction state topic.

**Why it matters**: Similar to `min.insync.replicas` but specifically for transaction metadata.

**Production**: Set to 2 for stronger guarantees.

</details>

---

## ZooKeeper Settings (in Broker)

<details>
<summary><strong>ğŸ”— zookeeper.connect=localhost:2181</strong></summary>

**What it does**: Connection string to ZooKeeper.

**Format**: `host1:port1,host2:port2,host3:port3/chroot`

**Why it matters**:
- Brokers register here and coordinate cluster operations
- All 3 brokers point to same ZooKeeper

**Our Setup**: Single ZooKeeper on `localhost:2181`

**Production Multi-Node**:
```properties
zookeeper.connect=zk1.prod.com:2181,zk2.prod.com:2181,zk3.prod.com:2181/kafka-prod

# The /kafka-prod is a "chroot" - namespaces Kafka data
```

**Test It**:
```bash
# Verify broker registration
kafka/bin/zookeeper-shell.sh localhost:2181 ls /brokers/ids

# Expected: [0, 1, 2]
```

</details>

<details>
<summary><strong>â±ï¸ zookeeper.connection.timeout.ms=18000</strong></summary>

**What it does**: Timeout (18 seconds) for establishing ZooKeeper connection.

**Why it matters**:
- If broker can't connect within 18s â†’ fails to start
- Protects against hanging on unreachable ZooKeeper

**Production**: Increase if network is slow/unreliable.

</details>

---

## Group Coordinator Settings

<details>
<summary><strong>âš–ï¸ group.initial.rebalance.delay.ms=0</strong></summary>

**What it does**: Delay before first rebalance when consumer group forms.

**Why it matters**: Prevents multiple rebalances during consumer startup.

**Rebalance Process**:
```
Consumer 1 joins â†’ Kafka waits (delay) for more consumers
Consumer 2 joins â†’ Still waiting...
Consumer 3 joins â†’ Delay expires â†’ ONE rebalance for all 3

vs.

delay=0:
Consumer 1 joins â†’ Rebalance 1
Consumer 2 joins â†’ Rebalance 2
Consumer 3 joins â†’ Rebalance 3  (wasteful!)
```

**Our Setup**: `0` = rebalance immediately (good for development)
**Production**: `3000` (3 seconds) to avoid rebalance storms during rolling restarts

**Test It**:
```bash
# Start consumers with delay to see rebalance behavior
# Terminal 1:
kafka/bin/kafka-console-consumer.sh \
  --topic test \
  --group demo-group \
  --bootstrap-server localhost:9092

# Terminal 2 (wait 5 seconds):
kafka/bin/kafka-console-consumer.sh \
  --topic test \
  --group demo-group \
  --bootstrap-server localhost:9092

# With delay=0: 2 rebalances
# With delay=3000: 1 rebalance (if both start within 3s)
```

</details>

---

## Configuration Differences Across Brokers

**What Changes Between Brokers**:

| Setting | Broker 0 | Broker 1 | Broker 2 | Why Different? |
|---------|----------|----------|----------|----------------|
| `broker.id` | 0 | 1 | 2 | Unique identifier required |
| `listeners` | 9092 | 9093 | 9094 | Can't bind to same port |
| `advertised.listeners` | 9092 | 9093 | 9094 | Clients need distinct addresses |
| `log.dirs` | `/tmp/kafka-logs-0` | `/tmp/kafka-logs-1` | `/tmp/kafka-logs-2` | Prevent data corruption |

**Everything else is identical** to ensure consistent cluster behavior.

---

## M1 Mac Optimization Tips

### Memory Tuning for 16GB RAM

**Broker JVM Settings** (create `kafka/bin/kafka-server-start-optimized.sh`):
```bash
#!/bin/bash
export KAFKA_HEAP_OPTS="-Xmx2G -Xms2G"  # 2GB heap per broker
export KAFKA_JVM_PERFORMANCE_OPTS="-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35"

exec $(dirname $0)/kafka-server-start.sh "$@"
```

**Memory Allocation Strategy** (16GB total):
```
System/macOS:        4 GB
ZooKeeper:          512 MB
Broker 0:             2 GB
Broker 1:             2 GB
Broker 2:             2 GB
Other apps:           3 GB
Buffers/Cache:      2.5 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:               16 GB
```

### Thread Tuning for M1 Architecture

**M1 Pro/Max Configuration**:
```properties
# M1 has 8-10 cores (varies by model)
# Optimize for efficiency + performance cores

num.network.threads=4              # Balance network I/O
num.io.threads=6                   # Leverage fast SSD
num.recovery.threads.per.data.dir=4  # Fast startup
```

### Disk I/O Optimization

**M1 Unified Memory + SSD Benefits**:
```properties
# M1 SSD is extremely fast - can be more aggressive
log.flush.interval.messages=10000  # Flush less frequently
log.flush.interval.ms=1000         # 1 second flush interval
```

**âš ï¸ Warning**: Only for local testing! Production should rely on OS page cache.

### Monitoring M1 Performance

```bash
# CPU usage per broker
top -pid $(pgrep -f "kafka.Kafka.*server-0")

# Memory pressure
memory_pressure

# Disk I/O
iostat -w 2

# Java heap usage
jstat -gc $(pgrep -f "kafka.Kafka.*server-0") 1000
```

---

## Common Mistakes & Troubleshooting

### âŒ Mistake 1: Setting Replication Factor > Broker Count

```bash
# This WILL FAIL:
kafka/bin/kafka-topics.sh --create \
  --topic test \
  --partitions 3 \
  --replication-factor 5 \  # âŒ Only 3 brokers!
  --bootstrap-server localhost:9092

# Error: Replication factor: 5 larger than available brokers: 3
```

**Fix**: Replication factor â‰¤ broker count.

---

### âŒ Mistake 2: Forgetting Unique log.dirs

```properties
# server-0.properties
log.dirs=/tmp/kafka-logs

# server-1.properties
log.dirs=/tmp/kafka-logs  # âŒ SAME DIRECTORY!

# Result: Data corruption, brokers fighting over same files
```

**Fix**: Always use unique directories per broker.

---

### âŒ Mistake 3: Using /tmp/ in Production

```bash
# After Mac reboot:
ls /tmp/kafka-logs-0
# ls: /tmp/kafka-logs-0: No such file or directory

# All data GONE! ğŸ˜±
```

**Fix**: Use permanent storage:
```properties
log.dirs=/var/lib/kafka/data
dataDir=/var/lib/zookeeper
```

---

### âŒ Mistake 4: Not Cleaning Up After Testing

```bash
# After 1 week of testing:
du -sh /tmp/kafka-logs-*
# 50G  /tmp/kafka-logs-0
# 50G  /tmp/kafka-logs-1
# 50G  /tmp/kafka-logs-2
# Total: 150GB on SSD!
```

**Fix**: Regular cleanup script:
```bash
# scripts/cleanup-all.sh
#!/bin/bash
kafka/bin/kafka-server-stop.sh
sleep 5
rm -rf /tmp/kafka-logs-*
rm -rf /tmp/zookeeper
```

---

### âŒ Mistake 5: Port Already in Use

```bash
kafka/bin/kafka-server-start.sh config/server-0.properties
# ERROR: Address already in use (bind failed) (java.net.BindException)
```

**Fix**:
```bash
# Find what's using the port
lsof -i :9092

# Kill old broker process
kill -9 <PID>

# Or use different ports in listeners
```

---

## What Happens When...

### ğŸ” What happens when you set replication factor > broker count?

**Scenario**:
```bash
kafka/bin/kafka-topics.sh --create \
  --topic test \
  --replication-factor 5 \  # Only 3 brokers exist
  --bootstrap-server localhost:9092
```

**Result**: âŒ **ERROR: Replication factor exceeds available brokers**

**Why**: Can't create 5 copies with only 3 brokers. Kafka needs unique brokers for each replica.

**Visual**:
```
Need 5 replicas:  [Replica 1] [Replica 2] [Replica 3] [Replica 4] [Replica 5]
Only 3 brokers:   [Broker 0]  [Broker 1]  [Broker 2]  [???]       [???]
                                                        â†‘
                                                   Not enough!
```

---

### ğŸ” What happens when min.insync.replicas > replication factor?

**Scenario**:
```properties
default.replication.factor=2
min.insync.replicas=3  # âŒ Impossible!
```

**Result**: âœ… Topic creates, but **writes FAIL**:
```
ERROR: NotEnoughReplicasException: Number of insync replicas = 2 < min.insync.replicas = 3
```

**Why**: Can't wait for 3 replicas when only 2 exist.

---

### ğŸ” What happens when you stop the leader broker?

**Scenario**:
```bash
# Check current leader
kafka/bin/kafka-topics.sh --describe --topic test --bootstrap-server localhost:9092
# Partition 0: Leader: 0  Replicas: 0,1  Isr: 0,1

# Stop broker 0
kill -9 $(pgrep -f "kafka.Kafka.*server-0")
```

**Result**: âœ… **Automatic failover!**
```
1. Broker 0 dies
2. ZooKeeper detects failure (no heartbeat)
3. Broker 1 elected as new leader (from ISR)
4. Producers/consumers automatically reconnect to Broker 1
5. No data loss (replica had all data)

New state:
# Partition 0: Leader: 1  Replicas: 0,1  Isr: 1
```

**Visual**:
```
Before:
  Broker 0 (LEADER) â”€â”€replicatesâ”€â”€> Broker 1 (FOLLOWER)
     â†“ (clients write here)
  [Writes]

Broker 0 crashes:
  âŒ Broker 0 (DEAD)    Broker 1 (FOLLOWER)
                             â†“
                       (ZooKeeper promotes)
                             â†“
After:
  âŒ Broker 0 (DEAD)    Broker 1 (NEW LEADER)
                             â†“ (clients now write here)
                         [Writes]
```

---

### ğŸ” What happens when all replicas of a partition fail?

**Scenario**: Stop Broker 0 and Broker 1 (both hold replicas of partition 0)

**Result**: ğŸ’€ **Partition unavailable**
```
ERROR: LeaderNotAvailableException

Producers: FAIL (can't write)
Consumers: FAIL (can't read)
```

**Recovery**: Start ANY broker with that partition â†’ becomes leader.